{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSmRChNKDX3yTWYzzgqlsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e056347dfaf94921b8dc376d60862235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62e158af775a4b4fb2306ba92ccb0414",
              "IPY_MODEL_b79d7db5cfc84b60ac17ebbe37f5bb24",
              "IPY_MODEL_2a69ccb61f8b4afe8d747a8d58807576"
            ],
            "layout": "IPY_MODEL_50b16b6f089747a4aaa49375a53b0714"
          }
        },
        "62e158af775a4b4fb2306ba92ccb0414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e542179c194417d8cc8c2cfe4ac58c9",
            "placeholder": "​",
            "style": "IPY_MODEL_551aaffc51244b74ba4db84878c4a579",
            "value": "Map: 100%"
          }
        },
        "b79d7db5cfc84b60ac17ebbe37f5bb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3daa5595f3ae473b92d2a3208ab38e0d",
            "max": 960,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2eedff59b51c46568c982dabf71bcb73",
            "value": 960
          }
        },
        "2a69ccb61f8b4afe8d747a8d58807576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d67f02e241e4718b57f65a6284a9b4c",
            "placeholder": "​",
            "style": "IPY_MODEL_2d2219c7bf5b4da9b535f97bf01c6738",
            "value": " 960/960 [00:02&lt;00:00, 462.65 examples/s]"
          }
        },
        "50b16b6f089747a4aaa49375a53b0714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e542179c194417d8cc8c2cfe4ac58c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551aaffc51244b74ba4db84878c4a579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3daa5595f3ae473b92d2a3208ab38e0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eedff59b51c46568c982dabf71bcb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d67f02e241e4718b57f65a6284a9b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2219c7bf5b4da9b535f97bf01c6738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mapcrafter2048/Literature-Review-Generator-ML-17/blob/main/main_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " data = load_dataset(\"scillm/scientific_papers-archive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhQnYlZvJcBi",
        "outputId": "dd4657b7-ebdc-4e1d-a2aa-1dc4d2d81643"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, AdamW\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set logging level\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# Constants\n",
        "DATASET_SIZE = 2400000\n",
        "SAMPLE_FRACTION = 0.004\n",
        "TRAIN_TEST_SPLIT = 0.1\n",
        "MAX_INPUT_LENGTH = 512  # Use 512 for T5\n",
        "MIN_TARGET_LENGTH = 5\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "MAX_EPOCHS = 2\n",
        "MODEL_CHECKPOINT = \"t5-small\"\n",
        "\n",
        "# Load dataset\n",
        "data = load_dataset(\"scillm/scientific_papers-archive\")\n",
        "\n",
        "# Downsample the dataset to 0.4%\n",
        "sample_size = int(DATASET_SIZE * SAMPLE_FRACTION)\n",
        "data = data.shuffle(seed=42)\n",
        "sampled_data = data['train'].train_test_split(train_size=sample_size, seed=42)['train']\n",
        "\n",
        "# Split the dataset into train and test\n",
        "train_test_data = sampled_data.train_test_split(test_size=TRAIN_TEST_SPLIT, seed=42)\n",
        "train_data = train_test_data['train']\n",
        "test_data = train_test_data['test']\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"input\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"output\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
        "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
        "\n",
        "# Convert to PyTorch Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.input_ids = torch.tensor(data[\"input_ids\"])\n",
        "        self.attention_mask = torch.tensor(data[\"attention_mask\"])\n",
        "        self.labels = torch.tensor(data[\"labels\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(tokenized_train)\n",
        "test_dataset = CustomDataset(tokenized_test)\n",
        "\n",
        "# Convert to DataLoader\n",
        "def create_dataloader(dataset, batch_size):\n",
        "    return DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "train_dataloader = create_dataloader(train_dataset, BATCH_SIZE)\n",
        "test_dataloader = create_dataloader(test_dataset, BATCH_SIZE)\n",
        "\n",
        "# Setup for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(test_dataloader)\n",
        "\n",
        "# Training and evaluation\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{MAX_EPOCHS}\")\n",
        "    train_loss = train()\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    eval_loss = evaluate()\n",
        "    print(f\"Evaluation Loss: {eval_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "e056347dfaf94921b8dc376d60862235",
            "62e158af775a4b4fb2306ba92ccb0414",
            "b79d7db5cfc84b60ac17ebbe37f5bb24",
            "2a69ccb61f8b4afe8d747a8d58807576",
            "50b16b6f089747a4aaa49375a53b0714",
            "0e542179c194417d8cc8c2cfe4ac58c9",
            "551aaffc51244b74ba4db84878c4a579",
            "3daa5595f3ae473b92d2a3208ab38e0d",
            "2eedff59b51c46568c982dabf71bcb73",
            "7d67f02e241e4718b57f65a6284a9b4c",
            "2d2219c7bf5b4da9b535f97bf01c6738"
          ]
        },
        "id": "eD2U9YU9LWFN",
        "outputId": "3bf57be6-35fd-4fbe-bfc0-7eefe2aa8dcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/960 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e056347dfaf94921b8dc376d60862235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4112: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1080 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
            "Training: 100%|██████████| 1080/1080 [06:01<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.7910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 120/120 [00:13<00:00,  8.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 2.3611\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1080/1080 [06:01<00:00,  2.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.4513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 120/120 [00:13<00:00,  8.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 2.2732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Constants\n",
        "MODEL_CHECKPOINT = \"t5-small\"\n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 128\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Ensure model is in evaluation mode and on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Generating summaries\n",
        "def generate_summaries(input_texts):\n",
        "    # Tokenize the input texts\n",
        "    inputs = tokenizer(input_texts, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\").to(device)\n",
        "\n",
        "    # Generate summaries\n",
        "    summaries = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        num_beams=4,               # Beam search for better summaries\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated summaries\n",
        "    return tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "sample_inputs = [\n",
        "    \"Quantum computing is revolutionizing the field of computation by leveraging quantum mechanics to solve problems beyond the reach of classical computers. This paper offers a comprehensive review of recent advancements in quantum computing, focusing on innovations in quantum hardware such as superconducting and topological qubits, and notable quantum algorithms including Shor’s and Grover’s algorithms that provide significant speedups over classical methods. We also examine progress in error correction techniques like surface codes and cat codes, which are crucial for managing quantum decoherence and scaling quantum systems. Additionally, the paper explores the transformative applications of quantum computing in areas such as cryptography, where it poses both challenges and opportunities, material science with its potential for groundbreaking simulations, and artificial intelligence, where quantum algorithms could enhance machine learning and optimization tasks. Despite these advancements, the field faces ongoing challenges, including the need for scalable systems and practical applications, making future research essential for realizing quantum computing’s full potential.\",\n",
        "\n",
        "]\n",
        "generated_summaries = generate_summaries(sample_inputs)\n",
        "\n",
        "# Print results\n",
        "for text, summary in zip(sample_inputs, generated_summaries):\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Summary: {summary}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QRCSQhjOz2_",
        "outputId": "d1b7edc1-9b88-4522-f7ab-25358f425922"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Quantum computing is revolutionizing the field of computation by leveraging quantum mechanics to solve problems beyond the reach of classical computers. This paper offers a comprehensive review of recent advancements in quantum computing, focusing on innovations in quantum hardware such as superconducting and topological qubits, and notable quantum algorithms including Shor’s and Grover’s algorithms that provide significant speedups over classical methods. We also examine progress in error correction techniques like surface codes and cat codes, which are crucial for managing quantum decoherence and scaling quantum systems. Additionally, the paper explores the transformative applications of quantum computing in areas such as cryptography, where it poses both challenges and opportunities, material science with its potential for groundbreaking simulations, and artificial intelligence, where quantum algorithms could enhance machine learning and optimization tasks. Despite these advancements, the field faces ongoing challenges, including the need for scalable systems and practical applications, making future research essential for realizing quantum computing’s full potential.\n",
            "Summary: computing is revolutionizing the field of computation by leveraging quantum mechanics to solve problems beyond classical computers. This paper offers comprehensive review of recent advancements in quantum computing. focusing on innovations in quantum hardware such as superconducting and topological qubits, and notable quantum algorithms including Shor’s and Grover’s algorithms.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}