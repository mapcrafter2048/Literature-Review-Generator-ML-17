{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhikxjsBsMzTi0sxpgmnol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mapcrafter2048/Literature-Review-Generator-ML-17/blob/main/Final_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ou5nIKAET6hc",
        "outputId": "dc360424-7b4a-483d-fe5e-dff24cb2ce02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow",
                  "requests"
                ]
              },
              "id": "17e7f199786a499c9badea85d37fcb5c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, AdamW\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set logging level\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# Constants\n",
        "DATASET_SIZE = 2400000\n",
        "SAMPLE_FRACTION = 0.004\n",
        "TRAIN_TEST_SPLIT = 0.1\n",
        "MAX_INPUT_LENGTH = 512  # Use 512 for T5\n",
        "MIN_TARGET_LENGTH = 5\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "MAX_EPOCHS = 2\n",
        "MODEL_CHECKPOINT = \"t5-small\"\n",
        "SLIDING_WINDOW_OVERLAP = 128  # Adjust as needed\n",
        "\n",
        "# Load dataset\n",
        "data = load_dataset(\"scillm/scientific_papers-archive\")"
      ],
      "metadata": {
        "id": "6RKylTjfeQ4Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nnpCjZseSNPo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Downsample the dataset to 0.4%\n",
        "sample_size = int(DATASET_SIZE * SAMPLE_FRACTION)\n",
        "data = data.shuffle(seed=42)\n",
        "sampled_data = data['train'].train_test_split(train_size=sample_size, seed=42)['train']\n",
        "\n",
        "# Split the dataset into train and test\n",
        "train_test_data = sampled_data.train_test_split(test_size=TRAIN_TEST_SPLIT, seed=42)\n",
        "train_data = train_test_data['train']\n",
        "test_data = train_test_data['test']\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Sliding window approach for long documents\n",
        "def sliding_window(text, max_length, overlap):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    segments = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        end = min(start + max_length, len(tokens))\n",
        "        segment = tokens[start:end]\n",
        "        decoded_segment = tokenizer.decode(segment, skip_special_tokens=True)\n",
        "        segments.append(decoded_segment)\n",
        "        if end == len(tokens):\n",
        "            break\n",
        "        start += max_length - overlap\n",
        "    return segments\n",
        "\n",
        "# De-duplication function\n",
        "def remove_duplicates(text_list):\n",
        "    \"\"\"\n",
        "    Remove duplicate sentences from a list of sentences.\n",
        "    Args:\n",
        "        text_list (list): List of sentences or text chunks.\n",
        "    Returns:\n",
        "        list: List with duplicate sentences removed.\n",
        "    \"\"\"\n",
        "    unique_text = []\n",
        "    vectorizer = TfidfVectorizer().fit_transform(text_list)\n",
        "    cosine_sim = cosine_similarity(vectorizer)\n",
        "    for i in range(len(text_list)):\n",
        "        if not any(cosine_sim[i, j] > 0.9 for j in range(i)):\n",
        "            unique_text.append(text_list[i])\n",
        "    return unique_text\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"input\"]]\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_labels = []\n",
        "\n",
        "    for input_text in inputs:\n",
        "        encoded_segments = tokenizer(input_text, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"np\")\n",
        "        all_input_ids.append(encoded_segments[\"input_ids\"].tolist())\n",
        "        all_attention_masks.append(encoded_segments[\"attention_mask\"].tolist())\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"output\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"np\")\n",
        "        for label in labels[\"input_ids\"]:\n",
        "            all_labels.append(label.tolist())\n",
        "\n",
        "    model_inputs = {\n",
        "        \"input_ids\": all_input_ids,\n",
        "        \"attention_mask\": all_attention_masks,\n",
        "        \"labels\": all_labels,\n",
        "    }\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_train = train_data.map(preprocess_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
        "tokenized_test = test_data.map(preprocess_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
        "\n",
        "# Fix column lengths\n",
        "def fix_column_length(dataset, column_name, expected_length):\n",
        "    def fix_length(example):\n",
        "        example[column_name] = example[column_name] + [0] * (expected_length - len(example[column_name]))\n",
        "        return example\n",
        "\n",
        "    return dataset.map(lambda example: fix_length(example), batched=True)\n",
        "\n",
        "train_data_fixed = fix_column_length(tokenized_train, \"input_ids\", MAX_INPUT_LENGTH)\n",
        "test_data_fixed = fix_column_length(tokenized_test, \"input_ids\", MAX_INPUT_LENGTH)\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.input_ids = torch.tensor(data[\"input_ids\"])\n",
        "        self.attention_mask = torch.tensor(data[\"attention_mask\"])\n",
        "        self.labels = torch.tensor(data[\"labels\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(train_data_fixed)\n",
        "test_dataset = CustomDataset(test_data_fixed)\n",
        "\n",
        "# DataLoader creation\n",
        "def create_dataloader(dataset, batch_size):\n",
        "    return DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "train_dataloader = create_dataloader(train_dataset, BATCH_SIZE)\n",
        "test_dataloader = create_dataloader(test_dataset, BATCH_SIZE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        if labels.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        input_ids = input_ids.view(input_ids.size(0), -1)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            if labels.shape[0] == 0:\n",
        "                continue\n",
        "\n",
        "            input_ids = input_ids.view(input_ids.size(0), -1)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(test_dataloader)\n",
        "\n",
        "# Training and evaluation\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{MAX_EPOCHS}\")\n",
        "    train_loss = train()\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    eval_loss = evaluate()\n",
        "    print(f\"Evaluation Loss: {eval_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxAL6lieiMa",
        "outputId": "f1d2bf7c-4421-4d4e-e2fd-81e4fd9fcb7f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1080/1080 [06:05<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.7920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 120/120 [00:14<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 2.3616\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1080/1080 [06:06<00:00,  2.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.4487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 120/120 [00:14<00:00,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 2.2718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary generation with de-duplication\n",
        "def generate_summary(text, model, tokenizer, max_input_length=512, max_target_length=128):\n",
        "    \"\"\"\n",
        "    Generates a summary for a given text using the specified model and tokenizer.\n",
        "    Args:\n",
        "        text (str): The input text to summarize.\n",
        "        model (transformers.PreTrainedModel): The pre-trained model for summarization.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer for the model.\n",
        "        max_input_length (int): Maximum input length for the model.\n",
        "        max_target_length (int): Maximum output length for the generated summary.\n",
        "    Returns:\n",
        "        str: The generated summary.\n",
        "    \"\"\"\n",
        "    def sliding_window(text, max_length, overlap):\n",
        "        tokens = tokenizer.encode(text, truncation=False)\n",
        "        segments = []\n",
        "        start = 0\n",
        "        while start < len(tokens):\n",
        "            end = min(start + max_length, len(tokens))\n",
        "            segment = tokens[start:end]\n",
        "            decoded_segment = tokenizer.decode(segment, skip_special_tokens=True)\n",
        "            segments.append(decoded_segment)\n",
        "            if end == len(tokens):\n",
        "                break\n",
        "            start += max_length - overlap\n",
        "        return segments\n",
        "\n",
        "    segments = sliding_window(text, max_input_length, SLIDING_WINDOW_OVERLAP)\n",
        "    summaries = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for segment in segments:\n",
        "            encoded_segment = tokenizer(segment, return_tensors=\"pt\", max_length=max_input_length, truncation=True, padding=\"max_length\").to(device)\n",
        "            outputs = model.generate(\n",
        "                input_ids=encoded_segment[\"input_ids\"],\n",
        "                attention_mask=encoded_segment[\"attention_mask\"],\n",
        "                max_length=max_target_length,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            summaries.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    combined_summary = ' '.join(summaries)\n",
        "    summary_sentences = combined_summary.split('. ')\n",
        "    unique_summary_sentences = remove_duplicates(summary_sentences)\n",
        "\n",
        "    return '. '.join(unique_summary_sentences)"
      ],
      "metadata": {
        "id": "ngEte-8ueosg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"Introduction\n",
        "NASA, the United States' premier space agency, has been at the forefront of space exploration and scientific discovery since its establishment in 1958. Over the decades, NASA's missions have significantly expanded human understanding of space, technology, and Earth. This paper provides an overview of key NASA missions, emphasizing their achievements, scientific contributions, and future directions.\n",
        "\n",
        "Historical Context and Major Milestones\n",
        "The Apollo Program\n",
        "One of NASA's most iconic endeavors is the Apollo program, which began with Apollo 1 in 1967 and culminated with Apollo 17 in 1972. The program's primary objective was to land humans on the Moon and return them safely to Earth. The Apollo 11 mission, launched on July 16, 1969, achieved this goal, with astronauts Neil Armstrong and Buzz Aldrin becoming the first humans to walk on the lunar surface. The program yielded a wealth of scientific data and lunar samples, greatly enhancing our understanding of lunar geology and the solar system's formation.\n",
        "\n",
        "The Space Shuttle Era\n",
        "Following the Apollo program, NASA embarked on the Space Shuttle program, which began with the launch of Columbia in 1981. The shuttle program was characterized by its reusable spacecraft, which facilitated the deployment and servicing of satellites, as well as the construction and operation of the International Space Station (ISS). Notable missions include the deployment of the Hubble Space Telescope in 1990, which has provided unprecedented views of the universe and contributed to our understanding of cosmic phenomena.\n",
        "\n",
        "The Mars Exploration Rovers\n",
        "In the early 2000s, NASA's focus shifted to Mars exploration. The Mars Exploration Rovers, Spirit and Opportunity, landed on Mars in January 2004. These rovers were designed to search for signs of past water activity and assess the planet's geological history. Their discoveries, including evidence of ancient water flows and diverse mineralogy, have been pivotal in shaping our understanding of Mars and its potential for past life.\n",
        "\n",
        "Recent Achievements\n",
        "The Mars Perseverance Rover\n",
        "Launched on July 30, 2020, NASA's Perseverance rover represents a significant advancement in Mars exploration. Equipped with a suite of scientific instruments and a helicopter named Ingenuity, Perseverance aims to search for signs of ancient life and collect samples for future return to Earth. Ingenuity's successful flights marked the first powered flight on another planet, showcasing technological advancements in autonomous aerial exploration.\n",
        "\n",
        "The James Webb Space Telescope\n",
        "Launched on December 25, 2021, the James Webb Space Telescope (JWST) is the successor to the Hubble Space Telescope. Positioned at the second Lagrange point (L2), JWST is designed to observe the universe in infrared wavelengths, allowing it to peer through dust clouds and observe the formation of the earliest galaxies. Its observations are expected to provide critical insights into cosmic origins, stellar evolution, and exoplanetary systems.\n",
        "\n",
        "The Artemis Program\n",
        "In 2020, NASA initiated the Artemis program with the goal of returning humans to the Moon and establishing a sustainable presence there. Artemis I, an uncrewed mission, successfully tested the Space Launch System (SLS) and the Orion spacecraft. The program aims to land the first woman and the next man on the lunar surface and establish a lunar base to facilitate future exploration of Mars.\n",
        "\n",
        "Technological Innovations\n",
        "NASA missions have driven numerous technological advancements. The development of the Space Shuttle’s reusable technology, the miniaturization of scientific instruments for rovers, and innovations in propulsion systems are just a few examples. Technologies developed for space missions have found applications in other fields, such as medical imaging, materials science, and telecommunications.\n",
        "\n",
        "Scientific Contributions\n",
        "NASA missions have provided invaluable scientific data across various domains:\n",
        "\n",
        "Planetary Science: Missions to Mars, Jupiter, and Saturn have offered insights into planetary atmospheres, geology, and potential habitability. The discovery of subsurface oceans on moons like Europa and Enceladus raises the possibility of extraterrestrial life.\n",
        "\n",
        "Astronomy and Cosmology: Telescopes like Hubble and JWST have revolutionized our understanding of the universe's structure, the formation of galaxies, and the nature of dark matter and dark energy.\n",
        "\n",
        "Earth Science: NASA’s Earth-observing satellites monitor climate change, natural disasters, and environmental changes. Missions like the Earth Observing System (EOS) have been instrumental in tracking atmospheric composition and land use.\n",
        "\n",
        "Future Prospects\n",
        "Looking ahead, NASA's future missions will continue to push the boundaries of exploration and scientific discovery. The Artemis program aims to establish a lunar base and use the Moon as a stepping stone for human missions to Mars. Additionally, NASA's plans for the Europa Clipper mission and the Dragonfly mission to Titan demonstrate a continued commitment to exploring the outer solar system and understanding the potential for life beyond Earth.\n",
        "\n",
        "Conclusion\n",
        "NASA’s missions have profoundly impacted science and technology, offering insights into the cosmos, advancing space technology, and inspiring global interest in exploration. From the historic Apollo Moon landings to the groundbreaking work of the James Webb Space Telescope, NASA’s achievements illustrate the agency’s dedication to expanding human knowledge and capability. As NASA continues to pursue ambitious goals, the future promises further discoveries and innovations that will continue to shape our understanding of the universe and our place within it.\"\"\"\n",
        "summary = generate_summary(text, model, tokenizer)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCjKgIBcYC-3",
        "outputId": "5060b327-c405-4395-dab5-1ddf78b96dbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". The Mars Perseverance rover is the first powered flight on another planet, showcasing technological advancements in autonomous aerial exploration. The Mars Perseverance Rovers, Spirit and Opportunity, landed on Mars in January 2004. The Perseverance rover was designed to search for signs of ancient life and assess the planet's geological history. The Perseverance rover was designed to search for signs of ancient life and collect samples for future return to Earth. The Perseverance rover ,,, and the formation of the earliest galaxies. Astronomy and Cosmology: Telescopes like Hubble and JWST have revolutionized our understanding of Mars and its potential for past life. Recent Achievements The Mars Perseverance Rover Launched on July 30, 2020, the James Webb Space Telescope (JWST) represents a significant advancement in our understanding of Mars and its potential for past life. Technological Innovations NASA missions have driven numerous technological advancements in autonomous aerial exploration. ,, and,, and the discovery of subsurface oceans on moons like Europa and Enceladus have revolutionized our understanding of the universe's structure, the formation of galaxies, and the nature of dark matter and dark energy. Future Prospects NASA’s missions have profoundly impacted science and technology, offering insights into the cosmos, advancing space technology, and inspiring global interest in exploration. Future Prospects NASA’s future missions will continue to push the boundaries of exploration and scientific discovery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"Quantum technology is an emerging field that promises to revolutionize various aspects of science, technology, and industry by exploiting the unique principles of quantum mechanics. At its core, quantum technology leverages phenomena such as superposition, entanglement, and quantum tunneling, which have no counterparts in classical physics. These phenomena enable unprecedented capabilities in computing, communication, and sensing, potentially transforming fields from cryptography to medicine.\n",
        "\n",
        "Quantum computing is perhaps the most well-known aspect of quantum technology. Unlike classical computers, which use bits to process information in binary states of 0s and 1s, quantum computers use quantum bits, or qubits. Qubits can exist simultaneously in multiple states due to the principle of superposition. This allows quantum computers to perform many calculations at once, exponentially increasing their processing power for certain types of problems. Additionally, qubits can be entangled, meaning the state of one qubit is dependent on the state of another, regardless of the distance between them. This entanglement enables quantum computers to solve complex problems that are currently intractable for classical computers, such as factoring large numbers, simulating molecular structures, and optimizing complex systems. The potential applications of quantum computing are vast and varied. In cryptography, quantum computers could break traditional encryption methods, which rely on the difficulty of factoring large numbers, necessitating the development of new, quantum-resistant encryption algorithms. In chemistry and material science, quantum computers can simulate the behavior of molecules at the quantum level, aiding in the design of new materials and drugs. In logistics and supply chain management, quantum algorithms can optimize routes and resources, improving efficiency and reducing costs. However, building a practical, large-scale quantum computer is a monumental challenge. Qubits are highly susceptible to environmental noise and decoherence, which can cause them to lose their quantum state. This fragility necessitates sophisticated error correction methods and extremely low-temperature environments to maintain qubit stability. Researchers are exploring various physical implementations of qubits, including superconducting circuits, trapped ions, and topological qubits, each with its own set of advantages and challenges.\n",
        "\n",
        "Quantum communication is another promising application of quantum technology, offering theoretically unbreakable encryption through quantum key distribution (QKD). QKD leverages the principles of quantum mechanics to create a secure communication channel. Any attempt to eavesdrop on the channel would disturb the quantum states being transmitted, alerting the communicating parties to the presence of an intruder. This level of security is unattainable with classical communication methods. Quantum networks, which extend QKD across larger distances, are currently being developed. These networks rely on quantum repeaters to overcome the loss and decoherence of quantum signals over long distances. Quantum communication could revolutionize secure communications, protecting sensitive information in fields such as finance, government, and healthcare. The development of quantum internet, where quantum information is transmitted and processed across a network of interconnected quantum devices, is a long-term goal that could enable new applications and services not possible with classical internet.\n",
        "\n",
        "Quantum sensing and metrology leverage quantum phenomena to achieve unprecedented levels of precision in measuring physical quantities. Quantum sensors can detect minute changes in magnetic fields, gravitational forces, and other environmental factors with extreme sensitivity. This capability has applications in various fields, including medical imaging, navigation, and geophysical exploration. For example, quantum sensors could improve the accuracy of MRI machines, enabling earlier detection of diseases. In navigation, quantum sensors could enhance the precision of gyroscopes and accelerometers, providing more accurate guidance for autonomous vehicles and spacecraft. In geophysical exploration, quantum sensors could detect subtle variations in the Earth's gravitational field, aiding in the discovery of natural resources. Despite the promise of quantum technology, significant technical and engineering challenges remain. Ensuring the stability and coherence of qubits, developing scalable quantum architectures, and integrating quantum systems with existing technologies are ongoing areas of research. Additionally, the field faces challenges in terms of standardization, interoperability, and the development of a skilled workforce capable of advancing quantum technology.\n",
        "\n",
        "The ethical and societal implications of quantum technology are also important considerations. The potential to break existing encryption methods raises concerns about data security and privacy. As quantum technology advances, it will be crucial to develop new cryptographic techniques and policies to safeguard sensitive information. Moreover, the development and deployment of quantum technologies will likely require significant investment and international collaboration, raising questions about access, equity, and the distribution of benefits. As with any transformative technology, it is essential to ensure that the advantages of quantum technology are widely shared and that potential risks are mitigated.\n",
        "\n",
        "The global race to develop quantum technology is well underway, with significant investments from governments, private companies, and research institutions. Countries such as the United States, China, and members of the European Union have launched national initiatives to advance quantum research and development. Major technology companies, including IBM, Google, and Microsoft, are also investing heavily in quantum computing and related technologies. This competitive landscape is driving rapid progress, but it also underscores the need for international cooperation and collaboration to address the complex challenges and maximize the benefits of quantum technology.\n",
        "\n",
        "In conclusion, quantum technology represents a frontier of scientific and technological innovation with the potential to transform a wide range of fields. Quantum computing promises to revolutionize information processing, enabling solutions to problems that are currently unsolvable. Quantum communication offers unparalleled security for transmitting information, and quantum sensing and metrology provide unprecedented precision in measurement. While significant challenges remain in realizing the full potential of quantum technology, ongoing research and development efforts are steadily advancing the field. As quantum technology continues to evolve, it will be essential to address the ethical, societal, and technical challenges to ensure that its benefits are broadly shared and its risks are effectively managed. The future of quantum technology holds exciting possibilities, and its impact on science, technology, and society is likely to be profound and far-reaching.\"\"\"\n",
        "summary = generate_summary(text, model, tokenizer)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxcjA2lYjhaf",
        "outputId": "93db02d2-cb08-42eb-bd5c-0e9ece9677fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". Quantum computing is an emerging field that promises to revolutionize various aspects of science, technology, and industry by exploiting the unique principles of quantum mechanics. At its core, quantum technology leverages phenomena such as superposition, entanglement, and quantum tunneling, which have no counterparts in classical physics. At its core, quantum technology leverages phenomena such as entanglement and entanglement to solve complex problems . Quantum communication is another promising application of quantum technology, offering theoretically unbreakable encryption through quantum key distribution (QKD). Quantum sensors can detect minute changes in magnetic fields, gravitational forces, and other environmental factors with extreme sensitivity. Quantum sensors could improve the accuracy of gyroscopes and accelerometers, providing more accurate guidance for autonomous vehicles and spacecraft. Quantum sensors . Quantum technology is a frontier of scientific and technological innovation with the potential to transform a wide range of fields. Quantum technology is a frontier of scientific and technological innovation with the potential to revolutionize information processing, enabling solutions to problems that are currently unsolvable. Quantum communication offers unparalleled security for transmitting information, and quantum sensing and metrology provide unprecedented precision in measurement. The potential to break existing encryption methods raises concerns about access, equity, and the distribution of benefits. The future of quantum technology holds exciting possibilities, and its . The future of quantum technology holds exciting possibilities, and its impact on science, technology, and society is likely to be profound and far-reaching\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"Abstract\n",
        "Artificial Intelligence (AI) and Machine Learning (ML) have experienced unprecedented growth in recent years, transforming numerous sectors including healthcare, finance, and transportation. This paper explores the current advancements in AI and ML, their practical applications, and the challenges associated with these technologies. By reviewing recent literature and case studies, the paper aims to provide a comprehensive overview of the state-of-the-art technologies, their implications, and future directions.\n",
        "\n",
        "Introduction\n",
        "Artificial Intelligence (AI) encompasses a broad range of technologies designed to perform tasks that typically require human intelligence. Machine Learning (ML), a subset of AI, focuses on algorithms that enable systems to learn from data and improve over time. The intersection of AI and ML has led to significant advancements, impacting various domains through applications such as natural language processing, computer vision, and autonomous systems.\n",
        "\n",
        "Advancements in AI and ML\n",
        "Deep Learning:\n",
        "Deep Learning (DL), a subset of ML, has revolutionized the field by leveraging neural networks with multiple layers to model complex patterns in data. Convolutional Neural Networks (CNNs) have achieved remarkable success in image and video recognition tasks. Recurrent Neural Networks (RNNs) and Transformers have significantly advanced natural language processing (NLP), enabling systems to understand and generate human language with greater accuracy.\n",
        "\n",
        "Reinforcement Learning:\n",
        "Reinforcement Learning (RL) involves training models through interactions with their environment to maximize cumulative rewards. Recent advancements in RL have led to breakthroughs in game playing, robotic control, and autonomous driving. Notable examples include AlphaGo's victory over human champions and advancements in self-driving car technologies.\n",
        "\n",
        "Generative Models:\n",
        "Generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have gained attention for their ability to generate new, synthetic data from learned distributions. These models are used in applications ranging from image synthesis to drug discovery and design.\n",
        "\n",
        "Transfer Learning:\n",
        "Transfer Learning involves applying knowledge gained from one domain to a different but related domain. This technique has made it possible to build high-performance models with relatively small amounts of data by leveraging pre-trained models.\n",
        "\n",
        "Applications of AI and ML\n",
        "Healthcare:\n",
        "AI and ML have made significant strides in healthcare, from diagnostics to personalized medicine. ML algorithms analyze medical images to detect diseases such as cancer and diabetic retinopathy. AI-driven platforms also assist in predicting patient outcomes and personalizing treatment plans. For example, IBM Watson for Oncology provides treatment recommendations based on vast amounts of medical literature and patient data.\n",
        "\n",
        "Finance:\n",
        "In the financial sector, AI and ML are used for fraud detection, algorithmic trading, and credit scoring. Machine learning models analyze transaction patterns to identify fraudulent activities and reduce financial risks. AI-driven robo-advisors provide personalized investment advice, optimizing portfolios based on individual risk profiles and market conditions.\n",
        "\n",
        "Transportation:\n",
        "Autonomous vehicles represent a significant application of AI and ML. Self-driving cars use a combination of computer vision, sensor data, and reinforcement learning to navigate and make driving decisions. Additionally, AI algorithms optimize logistics and supply chain management by predicting demand, routing deliveries, and managing inventory.\n",
        "\n",
        "Customer Service:\n",
        "AI-powered chatbots and virtual assistants have transformed customer service by providing 24/7 support and handling a wide range of inquiries. Natural language processing allows these systems to understand and respond to customer queries in real-time, improving customer satisfaction and operational efficiency.\n",
        "\n",
        "Challenges and Ethical Considerations\n",
        "Data Privacy and Security:\n",
        "The effectiveness of AI and ML heavily relies on the availability of large datasets. However, this raises concerns about data privacy and security. Ensuring that data is handled responsibly and that individuals' privacy is protected is crucial. Techniques such as differential privacy and federated learning are being developed to address these concerns.\n",
        "\n",
        "Bias and Fairness:\n",
        "AI systems can perpetuate and even exacerbate existing biases present in training data. Ensuring fairness and mitigating bias in AI systems is a critical challenge. Researchers are working on developing algorithms and practices to detect and correct bias, but achieving unbiased AI remains an ongoing challenge.\n",
        "\n",
        "Explainability and Transparency:\n",
        "Many advanced AI models, particularly deep learning networks, operate as \"black boxes,\" making it difficult to understand how they arrive at their decisions. Improving the explainability and transparency of AI systems is essential for trust and accountability, especially in high-stakes applications like healthcare and finance.\n",
        "\n",
        "Job Displacement:\n",
        "The automation of tasks through AI and ML raises concerns about job displacement and the future of work. While AI has the potential to create new job opportunities, it also necessitates reskilling and upskilling of the workforce to adapt to the changing job landscape.\n",
        "\n",
        "Future Directions\n",
        "Continued Innovation:\n",
        "The field of AI and ML is rapidly evolving, with ongoing research focusing on improving model efficiency, scalability, and generalization. Innovations such as quantum computing and neuromorphic engineering may further accelerate advancements in AI.\n",
        "\n",
        "Interdisciplinary Approaches:\n",
        "The integration of AI with other disciplines, such as cognitive science, neuroscience, and robotics, holds promise for developing more sophisticated and adaptable systems. Collaborative research across these fields can lead to new insights and applications.\n",
        "\n",
        "Ethical AI Development:\n",
        "As AI continues to evolve, it is imperative to prioritize ethical considerations in its development and deployment. Establishing guidelines, standards, and regulatory frameworks will be crucial in ensuring that AI technologies are used responsibly and for the benefit of society.\n",
        "\n",
        "Conclusion\n",
        "AI and ML have already made significant contributions across various domains, demonstrating their transformative potential. As technology continues to advance, it is essential to address the associated challenges and ethical considerations to ensure that AI and ML benefit society as a whole. Ongoing research, interdisciplinary collaboration, and responsible development practices will play a key role in shaping the future of these technologies.\"\"\"\n",
        "summary = generate_summary(text, model, tokenizer)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yveBUKu9j6RH",
        "outputId": "9edd57ce-6406-40b3-ddaf-6c298e63fcfe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and machine learning (ML) have experienced unprecedented growth in recent years, transforming numerous sectors including healthcare, finance, and transportation. Abstract Artificial Intelligence (AI) and Machine Learning (ML) have experienced unprecedented growth in the field by leveraging multiple layers to generate new, synthetic data and ML Healthcare: AI and ML have made significant strides in healthcare, from diagnostics to personalized medicine. AI-driven robo-advisors (VAEs) and Variational Autoencoders (VAEs) have gained attention for their ability to generate new, synthetic data from learned distributions. AI-driven robo-advisors provide personalized investment advice, optimizing portfolio management by predicting patient outcomes and personalizing treatment plans. Data Privacy and Security: AI systems can perpetuate and even exacerbate existing biases. in AI and ML: AI and ML are rapidly evolving, with ongoing research focusing on improving model efficiency, scalability, and generalization of AI and ML. Objectives and Objectives: As AI continues to evolve, it is imperative to prioritize ethical considerations in its development and deployment. Objectives and Objectives: As AI continues to evolve, it is imperative to prioritize ethical considerations in ensuring that AI and ML are used responsibly and for the benefit of society as a whole. Conclusion AI and ML have made significant contributions across various domains, demonstrating their transformative potential. Conclusion AI and ML technologies are used responsibly and for the benefit of society as \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwlWdBsGe2cG",
        "outputId": "062a32ea-02ca-4156-f08d-01ca262968be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "from transformers import TFT5ForConditionalGeneration\n",
        "\n",
        "# Assuming 'model' is your TFT5ForConditionalGeneration model\n",
        "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Final_summarisation_HF')"
      ],
      "metadata": {
        "id": "XS-ONTaQe5Dd"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}