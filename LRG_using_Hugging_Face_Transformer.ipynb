{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mapcrafter2048/Literature-Review-Generator-ML-17/blob/main/LRG_using_Hugging_Face_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpHhbQGV3aeH"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO0rHpZmP14h"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install huggingface-hub\n",
        "!pip install keras_nlp\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8SD0NsUP14l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXNZ0d19P14o"
      },
      "outputs": [],
      "source": [
        "TRAIN_TEST_SPLIT = 0.1\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "MIN_TARGET_LENGTH = 5\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "MAX_EPOCHS = 2\n",
        "MODEL_CHECKPOINT = \"t5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P--9D65BP14r"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"xsum\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ge-ZxrpP14s"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgWyFddM4que"
      },
      "outputs": [],
      "source": [
        "print(data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HajxgMhC_Ttx"
      },
      "outputs": [],
      "source": [
        "data = data.train_test_split(\n",
        "    train_size=TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXDxHguw2rVr"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o-oiYDa27IQ"
      },
      "outputs": [],
      "source": [
        "!pip show tensorflow\n",
        "!pip show tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwmP2HNvlJ__"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,AutoTokenizer\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8hNbRoSP14v"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP3uJe3dP14w"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = data.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqPF9AA4P14x"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "generation_dataset = (\n",
        "    tokenized_datasets[\"test\"]\n",
        "    .shuffle()\n",
        "    .select(list(range(200)))\n",
        "    .to_tf_dataset(\n",
        "        batch_size=BATCH_SIZE,\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpIplaj8JzKo"
      },
      "outputs": [],
      "source": [
        "optimizer = 'adam'\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq5mkV-8J6go"
      },
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "\n",
        "rouge_l = keras_nlp.metrics.RougeL()\n",
        "\n",
        "\n",
        "def metric_fn(eval_predictions):\n",
        "    predictions, labels = eval_predictions\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    for label in labels:\n",
        "        label[label < 0] = tokenizer.pad_token_id\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge_l(decoded_labels, decoded_predictions)\n",
        "\n",
        "    result = {\"RougeL\": result[\"f1_score\"]}\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bzQpTMV6J6go"
      },
      "outputs": [],
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
        ")\n",
        "\n",
        "callbacks = [metric_callback]\n",
        "\n",
        "model.fit(\n",
        "    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsQ_DJatJ6gs"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "\n",
        "summarizer(\n",
        "    data[\"test\"][0][\"document\"],\n",
        "    min_length=MIN_TARGET_LENGTH,\n",
        "    max_length=MAX_TARGET_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWOHrFLU6omA"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "\n",
        "article = \"The idea of India and China being close friends isn't new. Historically, they've acted like quarrelsome neighbours who fight and make up repeatedly. They share a long history of cultural and economic exchange dating back over two millennia. The Silk Road facilitated trade, and Buddhism, which started in India, found a significant following in China. These ancient ties laid a foundation of mutual respect and cultural affinity. During the colonial era, both countries faced subjugation by Western powers, fostering a sense of shared struggle \"\n",
        "\n",
        "summarizer(\n",
        "    article,\n",
        "    min_length=MIN_TARGET_LENGTH,\n",
        "    max_length=MAX_TARGET_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "499mfntwLtFV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FyvXkuet0nc"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library\n",
        "from transformers import TFT5ForConditionalGeneration\n",
        "\n",
        "# Assuming 'model' is your TFT5ForConditionalGeneration model\n",
        "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/summarisation_HF')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f0d6cd4ceb253e39b256fa9d77f54c927b7ca70e4289411a659862d2ff69b3ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}